{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0x05 Gradient descent and backpropagation\n",
    "\n",
    "In this tutorial we will cover the implementations and how-tos for interacting\n",
    "with the backpropagation engine of PyTorch.\n",
    "\n",
    "PyTorch is the go-to library for deep learning in Python especially if you are building a custom model on your own.\n",
    "You will be very likely be using PyTorch when you are doing your research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ’¡ **NOTE**: \n",
    ">\n",
    "> We assume you have already learnt the fundamentals of derivatives and gradients.\n",
    ">\n",
    "> If you need a quick recap, check out this explanation [here](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#optional-reading-vector-calculus-using-autograd) by the PyTorch team.\n",
    ">\n",
    "> Another suggested deep walkthrough is this [3blue1brown video](https://www.youtube.com/watch?v=tIeHLnjs5U8) on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Backpropagation\n",
    "\n",
    "One of PyTorch tensors' biggest difference with NumPy arrays is that they can track gradients.\n",
    "\n",
    "Two important ways to interact with it are `requires_grad` and the `.grad` property.\n",
    "Let us see it with a simple example.\n",
    "\n",
    "Consider this formula:\n",
    "$$\n",
    "y = w_1x^2 + w_2x + b\n",
    "$$\n",
    "\n",
    "where $x$ is the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **THINKING**\n",
    ">\n",
    "> - What is $\\frac{\\partial y}{\\partial w_1}$, $\\frac{\\partial y}{\\partial w_2}$, $\\frac{\\partial y}{\\partial b}$? Compute by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prove your computation, let us implement this formula in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0])\n",
    "\n",
    "# Identify the parameters that you need to compute gradients for\n",
    "# and set requires_grad=True\n",
    "w1 = torch.tensor([1.0], requires_grad=True)\n",
    "w2 = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([4.0], requires_grad=True)\n",
    "# You will see why we used an intermediate variable z here.\n",
    "z = w1 * torch.pow(x, 2) + w2 * x\n",
    "y = z + b\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute backpropagation\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([2.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# Gradient w.r.t. w1, w2, and b are stored in .grad of the tensors\n",
    "print(w1.grad)  # dy/dw1\n",
    "print(w2.grad)  # dy/dw2\n",
    "print(b.grad)    # dy/db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your computation correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you may want to also get $\\frac{dy}{dz}$ from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/zn3hm6l93hn5t3wcpy29ty1w0000gn/T/ipykernel_51474/385971827.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(z.grad)  # dy/dz\n"
     ]
    }
   ],
   "source": [
    "print(z.grad)  # dy/dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no ðŸ˜±, we cannot do it!\n",
    "\n",
    "This is because PyTorch does not update the gradients on **non-leaf** tensors. This makes sense because model parameters are leaf tensors. If you really **DO** want to compute the gradients of non-leaf tensors for specific use cases, you can use [`.retain_grad()`](https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“š **EXERCISE**\n",
    ">\n",
    "> - Define an expression on your own and get the gradients using backpropagation.\n",
    "> - Currently, our `y` is a scalar. Although loss is usually scalar in deep learning, what if we have a vector as `y`? How do we compute the gradients in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Your code here ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Toggling gradient tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are evaluating a model, you do not need to update the weights, and hence you do not need to track the gradients.\n",
    "\n",
    "Disabling gradient tracking helps reduce memory consumption and accelerate computations.\n",
    "\n",
    "In PyTorch, we use `torch.no_grad()` to disable gradient tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "x = torch.randn(1, 3, 224, 224) # batch size 1, 3 channels, 224x224 image\n",
    "with torch.no_grad():\n",
    "    y = resnet(x)\n",
    "print(y.shape) # 1000 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You may also notice that there is a `torch.eval()` method in PyTorch. It is different from `torch.no_grad()`. `eval()` is used to set the model in evaluation mode and it is mainly used for batch normalization and dropout layers. You will learn more about it in the following chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient descent and optimizing process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might already be using PyTorch optimizers to finetune your models. \n",
    "\n",
    "Optimizers update the model parameters using the gradients computed by backpropagation, and determines update process using certain algorithms.\n",
    "\n",
    "Today, we will use a simple example to show what is happening during the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the demonstration clear, suppose we have an EXTREMELY simple model, mapping a 3D vector to a scalar:\n",
    "$$\n",
    "y = w^Tx + b, x \\in \\mathbb{R}^3, w \\in \\mathbb{R}^3, b \\in \\mathbb{R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: tensor([1., 1., 1.], requires_grad=True), b: tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# We need gradient tracking for the parameters\n",
    "w = torch.ones(size=(3, ), requires_grad=True)\n",
    "b = torch.ones(size=(1, ), requires_grad=True)\n",
    "print(f\"w: {w}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the optimizers are implemented in the `torch.optim` module.\n",
    "\n",
    "Here we will use the `SGD` optimizer, which implements the stochastic gradient descent algorithm. It's basic form is given by:\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "where $\\eta$ is the learning rate and $L$ is the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([w, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it in action by supposing that we have a single input to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([10.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0, 3.0, 4.0])\n",
    "y = torch.dot(w, x) + b\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that, our dataset tell us that the ground-truth is $y = 2$ and we define our loss function as the squared error:\n",
    "$$\n",
    "L = \\frac{1}{2}(y - y_{pred})^2\n",
    "$$\n",
    "where $y_{pred}$ is the output of our model, $y$ is the ground-truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **THINKING**\n",
    ">\n",
    "> - Work out $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand.\n",
    "> - Based on your computation, why do we have a factor of $\\frac{1}{2}$ in the loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor([32.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the loss\n",
    "gt = torch.tensor([2.0])\n",
    "loss = 0.5 * torch.pow(gt - y, 2)\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad: tensor([16., 24., 32.]), w: tensor([1., 1., 1.], requires_grad=True)\n",
      "b.grad: tensor([8.]), b: tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Get the gradients\n",
    "loss.backward()\n",
    "print(f\"w.grad: {w.grad}, w: {w}\")\n",
    "print(f\"b.grad: {b.grad}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We step once through the optimization process and see how the model parameters are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad: tensor([16., 24., 32.]), w: tensor([0.8400, 0.7600, 0.6800], requires_grad=True)\n",
      "b.grad: tensor([8.]), b: tensor([0.9200], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"w.grad: {w.grad}, w: {w}\")\n",
    "print(f\"b.grad: {b.grad}, b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ¤” **THINKING**\n",
    ">\n",
    "> Did your hand-computation match the PyTorch output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ðŸ“š **EXERCISE**\n",
    ">\n",
    "> - Except the `SGD` optimizer, there are also other optimizers in PyTorch. Learn about them and try to implement them in the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Your code here ==="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
